{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shankar-Hadimani/PythonforDataScience/blob/master/4.%20MLP%20-%20Classification%20using%20Fashion%20MNIST%20dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem statement: Create a classification model for the Fashion MNIST\n",
        "\n",
        "The objective is to create a classification model for the Fashion MNIST dataset using a Multi-Layer Perceptron (MLP).\n",
        "\n",
        "We'll follow these steps:\n",
        "\n",
        "### 1. Data Preprocessing\n",
        "- **Loading the Data**: Fashion MNIST is a dataset of Zalando's article images, with 60,000 training samples and 10,000 test samples. Each sample is a 28x28 grayscale image, associated with a label from 10 classes.\n",
        "- **Normalization**: We normalize the pixel values (ranging from 0 to 255) to a scale of 0 to 1. This improves the training efficiency.\n",
        "- **Reshaping for MLP**: Since we are using an MLP, we need to reshape the 28x28 images into a flat array of 784 pixels.\n",
        "\n",
        "### 2. Building the MLP Model\n",
        "- **Dense Layers**: These are fully connected neural layers. The first layer needs to know the input shape (784 in this case).\n",
        "- **Activation Functions**: 'ReLU' is used for non-linear transformations. The final layer uses 'softmax' for a probability distribution over 10 classes.\n",
        "\n",
        "### 3. Compiling the Model\n",
        "- **Optimizer**: 'Adam' is a popular choice for its adaptive learning rate properties.\n",
        "- **Loss Function**: 'sparse_categorical_crossentropy' is suitable for multi-class classification problems.\n",
        "- **Metrics**: We'll use 'accuracy' to understand the performance.\n",
        "\n",
        "### 4. Training the Model\n",
        "- We train the model using the `fit` method, specifying epochs and batch size.\n",
        "\n",
        "### 5. Evaluating the Model\n",
        "- The `evaluate` method is used to test the model on the test set.\n",
        "\n",
        "The notebook contains one exercise in total:\n",
        "\n",
        "* [Exercise 1](#ex_1)"
      ],
      "metadata": {
        "id": "7-1RI2LOhzW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yKlxud86hcIh",
        "outputId": "bb6fbc2f-cc70-43a6-d47c-96cd986e56f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7608 - loss: 0.6894\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8567 - loss: 0.3996\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8724 - loss: 0.3525\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8833 - loss: 0.3204\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.8878 - loss: 0.3032\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8935 - loss: 0.2866\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8995 - loss: 0.2750\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9015 - loss: 0.2668\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2515\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9084 - loss: 0.2457\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8755 - loss: 0.3487\n",
            "Test accuracy: 0.8736000061035156\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1]\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for MLP input\n",
        "train_images = train_images.reshape((-1, 28*28))\n",
        "test_images = test_images.reshape((-1, 28*28))\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the model's accuracy on the Fashion MNIST dataset, we can experiment with various techniques. Here are some strategies:\n",
        "\n",
        "1. **Increase Model Complexity**: Add more layers or increase the number of neurons in each layer to capture more complex patterns in the data.\n",
        "\n",
        "2. **Regularization**: Implement dropout or L1/L2 regularization to reduce overfitting.\n",
        "\n",
        "3. **Advanced Optimizers**: Experiment with different optimizers like SGD or RMSprop.\n",
        "\n",
        "4. **Learning Rate Scheduling**: Adjust the learning rate during training.\n",
        "\n",
        "5. **Data Augmentation**: Although not typical for MLPs, slight modifications to the input data can make the model more robust.\n",
        "\n",
        "6. **Early Stopping**: Stop training when the validation accuracy stops improving.\n",
        "\n",
        "7. **Hyperparameter Tuning**: Experiment with different activation functions, batch sizes, and epochs.\n",
        "\n",
        "8. **Batch Normalization**: This can help in faster convergence and overall performance improvement.\n",
        "\n",
        "Let's modify the previous code to incorporate some of these strategies."
      ],
      "metadata": {
        "id": "yL1TJaEzjvVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Modified MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
        "model.add(BatchNormalization())  # Batch normalization layer\n",
        "model.add(Dropout(0.5))         # Dropout layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())  # Another batch normalization layer\n",
        "model.add(Dropout(0.5))         # Another dropout layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Train the model with validation split\n",
        "model.fit(train_images, train_labels, epochs=50, batch_size=64,\n",
        "          validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "GkitFh5aij7B",
        "outputId": "a7f2475c-2042-477d-c50a-437aab3ca53a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.6838 - loss: 0.9503 - val_accuracy: 0.8286 - val_loss: 0.4561\n",
            "Epoch 2/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8108 - loss: 0.5361 - val_accuracy: 0.8403 - val_loss: 0.4288\n",
            "Epoch 3/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8254 - loss: 0.4956 - val_accuracy: 0.8437 - val_loss: 0.4261\n",
            "Epoch 4/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8326 - loss: 0.4705 - val_accuracy: 0.8636 - val_loss: 0.3815\n",
            "Epoch 5/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8426 - loss: 0.4549 - val_accuracy: 0.8445 - val_loss: 0.4101\n",
            "Epoch 6/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8381 - loss: 0.4525 - val_accuracy: 0.8596 - val_loss: 0.3772\n",
            "Epoch 7/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8404 - loss: 0.4456 - val_accuracy: 0.8608 - val_loss: 0.3752\n",
            "Epoch 8/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8471 - loss: 0.4308 - val_accuracy: 0.8686 - val_loss: 0.3565\n",
            "Epoch 9/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8506 - loss: 0.4154 - val_accuracy: 0.8712 - val_loss: 0.3510\n",
            "Epoch 10/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8537 - loss: 0.4123 - val_accuracy: 0.8685 - val_loss: 0.3566\n",
            "Epoch 11/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.8535 - loss: 0.4068 - val_accuracy: 0.8664 - val_loss: 0.3624\n",
            "Epoch 12/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8566 - loss: 0.4045 - val_accuracy: 0.8639 - val_loss: 0.3630\n",
            "Epoch 13/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8547 - loss: 0.4113 - val_accuracy: 0.8716 - val_loss: 0.3433\n",
            "Epoch 14/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8548 - loss: 0.3995 - val_accuracy: 0.8635 - val_loss: 0.3562\n",
            "Epoch 15/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8620 - loss: 0.3867 - val_accuracy: 0.8788 - val_loss: 0.3294\n",
            "Epoch 16/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8641 - loss: 0.3775 - val_accuracy: 0.8767 - val_loss: 0.3355\n",
            "Epoch 17/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8647 - loss: 0.3809 - val_accuracy: 0.8777 - val_loss: 0.3345\n",
            "Epoch 18/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8639 - loss: 0.3759 - val_accuracy: 0.8748 - val_loss: 0.3377\n",
            "Epoch 19/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8672 - loss: 0.3693 - val_accuracy: 0.8770 - val_loss: 0.3327\n",
            "Epoch 20/50\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8679 - loss: 0.3681 - val_accuracy: 0.8786 - val_loss: 0.3329\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8713 - loss: 0.3567\n",
            "Test accuracy: 0.8687000274658203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy decreased slightly in this case. This outcome highlights an important aspect of machine learning: improvements in model architecture don't always lead to better performance, and sometimes simpler models can outperform more complex ones, especially on smaller datasets like Fashion MNIST.\n",
        "\n",
        "Here are a few additional steps you can take to try and improve the model's performance:\n",
        "\n",
        "1. **Adjust the Dropout Rate**: The dropout rate of 0.5 might be too high, causing the model to lose relevant information. Try reducing it to 0.3 or 0.2.\n",
        "\n",
        "2. **Fine-Tune the Model Complexity**: The addition of more neurons might have made the model too complex. Try reducing the number of neurons in the dense layers.\n",
        "\n",
        "3. **Experiment with Different Optimizers**: While Adam is a strong general-purpose optimizer, sometimes others like SGD (with a momentum) or RMSprop might yield better results for specific problems.\n",
        "\n",
        "4. **Modify the Learning Rate**: Adjusting the learning rate of the Adam optimizer could also lead to better results. A lower learning rate with more epochs can sometimes achieve better generalization.\n",
        "\n",
        "5. **Experiment with Batch Sizes**: Smaller or larger batch sizes can impact the model's ability to generalize and learn effectively.\n",
        "\n",
        "6. **Cross-Validation**: Instead of a single validation split, use k-fold cross-validation for a more robust estimate of model performance.\n",
        "\n",
        "Let's adjust the code with some of these suggestions."
      ],
      "metadata": {
        "id": "lGXeENRJjzVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the model architecture and training parameters\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.3))         # Reduced dropout rate\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))         # Reduced dropout rate\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with a modified optimizer\n",
        "model.compile(optimizer='adam',  # You can experiment with learning rate here\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with a different batch size\n",
        "model.fit(train_images, train_labels, epochs=50, batch_size=32,  # Smaller batch size\n",
        "          validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "T9z3khhbjrVp",
        "outputId": "25db7336-abfa-4193-b951-9f515c623e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6834 - loss: 0.8965 - val_accuracy: 0.8463 - val_loss: 0.4276\n",
            "Epoch 2/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8247 - loss: 0.4866 - val_accuracy: 0.8425 - val_loss: 0.4211\n",
            "Epoch 3/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8389 - loss: 0.4453 - val_accuracy: 0.8611 - val_loss: 0.3763\n",
            "Epoch 4/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8519 - loss: 0.4106 - val_accuracy: 0.8719 - val_loss: 0.3582\n",
            "Epoch 5/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8545 - loss: 0.3992 - val_accuracy: 0.8617 - val_loss: 0.3707\n",
            "Epoch 6/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8601 - loss: 0.3856 - val_accuracy: 0.8757 - val_loss: 0.3516\n",
            "Epoch 7/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.3762 - val_accuracy: 0.8742 - val_loss: 0.3559\n",
            "Epoch 8/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8663 - loss: 0.3686 - val_accuracy: 0.8746 - val_loss: 0.3434\n",
            "Epoch 9/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8696 - loss: 0.3586 - val_accuracy: 0.8740 - val_loss: 0.3381\n",
            "Epoch 10/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8740 - loss: 0.3436 - val_accuracy: 0.8800 - val_loss: 0.3316\n",
            "Epoch 11/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8693 - loss: 0.3508 - val_accuracy: 0.8802 - val_loss: 0.3321\n",
            "Epoch 12/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8745 - loss: 0.3328 - val_accuracy: 0.8798 - val_loss: 0.3260\n",
            "Epoch 13/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8794 - loss: 0.3268 - val_accuracy: 0.8834 - val_loss: 0.3333\n",
            "Epoch 14/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8800 - loss: 0.3302 - val_accuracy: 0.8838 - val_loss: 0.3344\n",
            "Epoch 15/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8828 - loss: 0.3201 - val_accuracy: 0.8789 - val_loss: 0.3248\n",
            "Epoch 16/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8817 - loss: 0.3206 - val_accuracy: 0.8823 - val_loss: 0.3376\n",
            "Epoch 17/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8837 - loss: 0.3092 - val_accuracy: 0.8836 - val_loss: 0.3230\n",
            "Epoch 18/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8857 - loss: 0.3117 - val_accuracy: 0.8859 - val_loss: 0.3213\n",
            "Epoch 19/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8845 - loss: 0.3121 - val_accuracy: 0.8813 - val_loss: 0.3192\n",
            "Epoch 20/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8879 - loss: 0.3011 - val_accuracy: 0.8841 - val_loss: 0.3280\n",
            "Epoch 21/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8849 - loss: 0.3056 - val_accuracy: 0.8848 - val_loss: 0.3200\n",
            "Epoch 22/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8915 - loss: 0.2900 - val_accuracy: 0.8878 - val_loss: 0.3182\n",
            "Epoch 23/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8915 - loss: 0.2969 - val_accuracy: 0.8855 - val_loss: 0.3222\n",
            "Epoch 24/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8880 - loss: 0.2968 - val_accuracy: 0.8880 - val_loss: 0.3218\n",
            "Epoch 25/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8922 - loss: 0.2895 - val_accuracy: 0.8857 - val_loss: 0.3346\n",
            "Epoch 26/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.2934 - val_accuracy: 0.8895 - val_loss: 0.3230\n",
            "Epoch 27/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8915 - loss: 0.2890 - val_accuracy: 0.8857 - val_loss: 0.3162\n",
            "Epoch 28/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8911 - loss: 0.2879 - val_accuracy: 0.8825 - val_loss: 0.3306\n",
            "Epoch 29/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8961 - loss: 0.2822 - val_accuracy: 0.8835 - val_loss: 0.3277\n",
            "Epoch 30/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8944 - loss: 0.2806 - val_accuracy: 0.8910 - val_loss: 0.3148\n",
            "Epoch 31/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8962 - loss: 0.2797 - val_accuracy: 0.8896 - val_loss: 0.3170\n",
            "Epoch 32/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8935 - loss: 0.2819 - val_accuracy: 0.8871 - val_loss: 0.3328\n",
            "Epoch 33/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8976 - loss: 0.2741 - val_accuracy: 0.8870 - val_loss: 0.3260\n",
            "Epoch 34/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8995 - loss: 0.2707 - val_accuracy: 0.8863 - val_loss: 0.3304\n",
            "Epoch 35/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8989 - loss: 0.2681 - val_accuracy: 0.8876 - val_loss: 0.3210\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8827 - loss: 0.3408\n",
            "Test accuracy: 0.8841000199317932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy has improved to 0.8778, which is a positive outcome. This result indicates that the adjustments made to the model architecture and training parameters were beneficial.\n",
        "\n",
        "However, achieving higher accuracy on a dataset like Fashion MNIST can be challenging, especially with a simple model like a Multi-Layer Perceptron (MLP). To potentially achieve even better results, consider the following additional steps:\n",
        "\n",
        "1. **Feature Engineering**: Although this is more limited with image data and MLPs, ensuring the input data is as informative and clean as possible is crucial.\n",
        "\n",
        "2. **Ensemble Methods**: Combine predictions from several models to improve accuracy. For example, train multiple MLPs with different architectures and average their predictions.\n",
        "\n",
        "3. **Convolutional Neural Networks (CNNs)**: For image data, CNNs are generally more effective than MLPs. They can capture spatial hierarchies in the data better due to their convolutional layers.\n",
        "\n",
        "4. **Hyperparameter Optimization**: Use techniques like grid search or random search to systematically explore different hyperparameter combinations.\n",
        "\n",
        "5. **Advanced Regularization Techniques**: Experiment with other regularization methods like L1 regularization or different dropout configurations.\n",
        "\n",
        "Let's adjust the code with some of these suggestions."
      ],
      "metadata": {
        "id": "rmWbPC5gmHZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Reshape data for CNN input\n",
        "train_images_cnn = train_images.reshape((-1, 28, 28, 1))\n",
        "test_images_cnn = test_images.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# Build a simple CNN model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model.add(MaxPooling2D((2, 2)))\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_images_cnn, train_labels, epochs=10, batch_size=64,\n",
        "              validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images_cnn, test_labels)\n",
        "\n",
        "print('CNN Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "BKPmmHqImJEi",
        "outputId": "b7b284e4-b6c6-4a45-8d0c-14455453683b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.7158 - loss: 0.8092 - val_accuracy: 0.8484 - val_loss: 0.4256\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 49ms/step - accuracy: 0.8687 - loss: 0.3742 - val_accuracy: 0.8732 - val_loss: 0.3515\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.8878 - loss: 0.3135 - val_accuracy: 0.8833 - val_loss: 0.3147\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 49ms/step - accuracy: 0.9002 - loss: 0.2760 - val_accuracy: 0.8906 - val_loss: 0.3033\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 48ms/step - accuracy: 0.9051 - loss: 0.2595 - val_accuracy: 0.8991 - val_loss: 0.2824\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - accuracy: 0.9125 - loss: 0.2366 - val_accuracy: 0.8997 - val_loss: 0.2794\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9190 - loss: 0.2180 - val_accuracy: 0.9057 - val_loss: 0.2614\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9260 - loss: 0.2017 - val_accuracy: 0.9054 - val_loss: 0.2679\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 48ms/step - accuracy: 0.9312 - loss: 0.1845 - val_accuracy: 0.9110 - val_loss: 0.2566\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 48ms/step - accuracy: 0.9386 - loss: 0.1664 - val_accuracy: 0.9131 - val_loss: 0.2445\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9041 - loss: 0.2684\n",
            "CNN Test accuracy: 0.9086999893188477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ex_1\"></a>\n",
        "## Exercise 1: Improve the accuracy of the MLP model\n",
        "1. Try different architectures and hyperparameters.\n",
        "2. Use regularization techniques like L1 or L2 regularization.\n",
        "3. Use dropout to reduce overfitting.\n",
        "\n",
        "Referans link: https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/"
      ],
      "metadata": {
        "id": "WACMOuHj3wp-"
      }
    }
  ]
}